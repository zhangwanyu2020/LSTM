# -*- coding: utf-8 -*-
"""LSTM_classifier.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EAv38W5hIWuPW4iTLllpGSiQMcdbBht1
"""

import json
import os
import numpy as np
import pandas as pd
os.environ['CUDA_LAUNCH_BLOCKING'] = "1" 
import torch
import pickle
import logging
from pathlib import Path
import tensorflow as tf

config = {
    "embedding_size": 128,
    "hidden_size": 256,
    "hidden_size2": 128,
    "num_layers": 2,
    "num_classes": 14,
    "vocab_size": 20000,
    "max_length": 16,
    "dropout_prob": 0.1,
    "batch_size": 64,
    "learning_rate": 1e-3,
    "epochs": 2
}

def load_pkl(pkl_path):
    with open(pkl_path, 'rb') as f:
        result = pickle.load(f)
    return result

def load_word2vec(word2vec_output,vocab_path,vocab_size=20000,embedding_size=128):

    word2vec_dict = load_pkl(word2vec_output)
    vocabs = open(vocab_path, encoding='utf-8').readlines()
    embedding_matrix = np.zeros((vocab_size+1, embedding_size))
    for line in vocabs[1:vocab_size+1]:# 第一个字符是空格，从第二个字符开始
        word_id = line.split()
        if len(word_id)==2:
            id, word = word_id
        embedding_vector = word2vec_dict.get(word)
        
        if embedding_vector is not None:
            embedding_matrix[int(id)]= embedding_vector

    return embedding_matrix

matrix = load_word2vec(word2vec_output='/content/drive/MyDrive/LSTM_P1/words_vectors.txt',
                       vocab_path='/content/drive/MyDrive/LSTM_P1/vocabs.txt')

class Classifier_2(torch.nn.Module):
   
    def __init__(self, config, embedding_matrix):
        # TODO: 定义网络结构，注意，如果想要自定义初始化参数，不要忘了实现
        super(Classifier_2, self).__init__()
        
        self.batch_size = config["batch_size"]
        self.embedding_matrix = embedding_matrix
        self.word_embedding = torch.nn.Embedding.from_pretrained(self.embedding_matrix)
        self.rnns = torch.nn.LSTM(input_size=config["embedding_size"],
                                  hidden_size=config["hidden_size"],
                                  num_layers=config["num_layers"],
                                  batch_first=True)
        print(self.rnns)
        self.linear1 = torch.nn.Linear(config["hidden_size"],config["hidden_size2"])
        self.linear2 = torch.nn.Linear(config["hidden_size2"],config["num_classes"])
        self.softmax = torch.nn.Softmax(dim=1)

    def init_state(self): 
        n = config["num_layers"] 
        h = config["hidden_size"]
        torch.set_default_tensor_type(torch.DoubleTensor)
        hs = torch.zeros(n, self.batch_size, h)#.cuda()# hidden state
        cs = torch.zeros(n, self.batch_size, h)#.cuda() # LSTM cell state
        return hs, cs
    
    def forward(self,input_x,mode='c'):
        # TODO: 定义前向过程，不要忘了写上你需要的参数，要在前向过程算出来loss，同时为了方便预测，不要忘了返回logits
        x = self.word_embedding(input_x)
        h0, c0 = self.init_state()
        output, (h_state, c_state) = self.rnns(x, (h0, c0))
        if mode == 'c':
            c_state = torch.sum(c_state,0)
            out = self.linear1(c_state)
        elif mode == 'mean':
            T = x.size()[1]
            mean_state = torch.sum(output, 1) / T
            out = self.linear1(mean_state)
        out = torch.sigmoid(out)
        out = self.linear2(out)
        logits = self.softmax(out)
        logits = torch.squeeze(logits, 0)
    
        return logits, output

embedding_matrix = torch.from_numpy(matrix)
print(embedding_matrix[10].shape)
# model = Classifier_2(config,embedding_matrix)
# dummy_input = torch.randint(1,20000,(config["batch_size"],config["max_length"]))
# logits,output= model(dummy_input,mode='c')

labels_to_indices = dict()
with open("/content/drive/MyDrive/LSTM_P1/labels.txt", encoding="utf-8", errors="ignore") as fp:
    for line in fp:
        id, label = line.split(' ')
        labels_to_indices[int(id)] = label.strip()
print(labels_to_indices)
indices_to_labels = dict(zip(labels_to_indices.values(), labels_to_indices.keys()))
print(indices_to_labels)

# Commented out IPython magic to ensure Python compatibility.
PAD_TOKEN = '[PAD]'
UNKNOWN_TOKEN = '[UNK]'

class Tokenizer:
    def __init__(self, vocab_path, max_size):
        self.word2id = {PAD_TOKEN: 0, UNKNOWN_TOKEN:1}
        self.id2word = {0: PAD_TOKEN, 1:UNKNOWN_TOKEN}
        self.count = 2

        with open(vocab_path, 'r', encoding='utf-8') as f:
            for line in f:
                pieces = line.split()
                if len(pieces) != 2:
                    continue

                w = pieces[1]
                if w==PAD_TOKEN:
                    raise Exception(r'[PAD] shouldn\'t be in the vocab file, '
                                    r'but %s is' % w)

                if w in self.word2id:
                    raise Exception('Duplicated word in vocabulary file: %s' % w)

                self.word2id[w] = self.count
                self.id2word[self.count] = w
                self.count += 1
                if max_size != 0 and self.count >= max_size:
                    print("max_size of vocab was specified as %i; we now have %i words. Stopping reading."
#                           % (max_size, self.count))
                    break

        print("Finished constructing vocabulary of %i total words. Last word added: %s" %
              (self.count, self.id2word[self.count - 1]))

    def word_to_id(self, word):
        if word not in self.word2id:
            return self.word2id[UNKNOWN_TOKEN]
        return self.word2id[word]

    def id_to_word(self, word_id):
        if word_id not in self.id2word:
            raise ValueError('Id not found in vocab: %d' % word_id)
        return self.id2word[word_id]

    def size(self):
        return self.count

vocab_path = '/content/drive/MyDrive/LSTM_P1/vocabs.txt'
max_size = 20000
vocab = Tokenizer(vocab_path,max_size)

def get_label_vec (dict, value):
    key = [k for k, v in dict.items() if v == value][0]
    label_onehot = [0] * len(dict)
    label_onehot[key] = 1
    return label_onehot


def get_key (dict, value):
    key = [k for k, v in dict.items() if v == value][0]
    return key

def to_var(x):
    if torch.cuda.is_available():
        return Variable(x).cuda()
    else:
        return Variable(x)
    

class AverageMeter(object):
    '''
    computes and stores the average and current value
    Example:
        >>> loss = AverageMeter()
        >>> for step,batch in enumerate(train_data):
        >>>     pred = self.model(batch)
        >>>     raw_loss = self.metrics(pred,target)
        >>>     loss.update(raw_loss.item(),n = 1)
        >>> cur_loss = loss.avg
    '''

    def __init__(self):
        self.reset()

    def reset(self):
        self.val = 0
        self.avg = 0
        self.sum = 0
        self.count = 0

    def update(self, val, n=1):
        self.val = val
        self.sum += self.val * n
        self.count += n
        self.avg = self.sum / self.count

logger = logging.getLogger()

def init_logger(log_file=None, log_file_level=logging.NOTSET):
    '''
    Example:
        >>> init_logger(log_file)
        >>> logger.info("abc'")
    '''
    if isinstance(log_file,Path):
        log_file = str(log_file)
    log_format = logging.Formatter("%(message)s")
    logger = logging.getLogger()
    logger.setLevel(logging.INFO)
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(log_format)
    logger.handlers = [console_handler]
    if log_file and log_file != '':
        file_handler = logging.FileHandler(log_file)
        file_handler.setLevel(log_file_level)
        file_handler.setFormatter(log_format)
        logger.addHandler(file_handler)
    return logger


class Metric:
    def __init__(self):
        pass

    def __call__(self, outputs, target):
        raise NotImplementedError

    def reset(self):
        raise NotImplementedError

    def value(self):
        raise NotImplementedError

    def name(self):
        raise NotImplementedError

class Accuracy(Metric):
    '''
    计算准确度
    可以使用topK参数设定计算K准确度
    Example:
        >>> metrics = Accuracy(**)
        >>> for epoch in range(epochs):
        >>>     metrics.reset()
        >>>     for batch in batchs:
        >>>         logits = model()
        >>>         metrics(logits,target)
        >>>         print(metrics.name(),metrics.value())
    '''
    def __init__(self,topK):
        super(Accuracy,self).__init__()
        self.topK = topK
        self.reset()

    def __call__(self, logits, target):
        _, pred = logits.topk(self.topK, 1, True, True)
        pred = pred.t()
        correct = pred.eq(target.view(1, -1).expand_as(pred))
        self.correct_k = correct[:self.topK].view(-1).float().sum(0)
        self.total = target.size(0)

    def reset(self):
        self.correct_k = 0
        self.total = 0

    def value(self):
        return float(self.correct_k)  / self.total

    def name(self):
        return 'accuracy'

class NewsDataset(torch.utils.data.Dataset):

    def __init__(self, vocab, data_path, max_len, labels_to_indices):
        df = pd.read_csv(data_path,names=['text','label'])
        train_data = []  
        pad_decoding = vocab.word_to_id(PAD_TOKEN)
        for line in df.iterrows(): 
            text = str(line[1][0])
            text_split = text.split()
            len_text = len(text_split)
            if len_text < max_len: 
                text_ids = [vocab.word_to_id(w) for w in text_split] + [pad_decoding]*(max_len - len_text)
            else:
                text_split = text_split[:max_len]
                text_ids = [vocab.word_to_id(w) for w in text_split]
            label = line[1][1]
            label_id = get_label_vec(labels_to_indices,label)
            label_id_sparse = get_key(labels_to_indices,label)
            train_data.append((text_ids,label_id,label_id_sparse))
        self.train_data = train_data

    def __len__(self):
        return len(self.train_data)
    
    def __getitem__(self, index):
        # TODO: 实现getitem方法，用于取出数据集的元素。torch的张量可以看作是numpy数组，所以可以直接用索引取元素，可以跳取
        text_ids, label_id, label_id_sparse = self.train_data[index]

        text_ids = np.asarray(text_ids)
        text_ids = torch.from_numpy(text_ids.astype('long'))

        label_id = np.asarray(label_id)
        label_id = torch.from_numpy(label_id.astype('double'))

        return text_ids, label_id, label_id_sparse

    
    # @classmethod
    # def processor(cls, batch_size):
    #     # TODO: 实现从文件中读取数据，并创建数据集的逻辑，这个类方法应当返回一个数据集，在外层可以直接调用`NewsDataset.processor(...)`来实例化数据集

def train(config):
    #TODO 定义超参数
    learning_rate = config["learning_rate"]
    batch_size = config["batch_size"]
    epochs = config["epochs"]
    num_eval_steps = 1000
    num_save_steps = 10000
    model_save_path = '/content/drive/MyDrive/LSTM_P1/config.json'
    label_path = '/content/drive/MyDrive/LSTM_P1/labels.txt'
    output_dir = '/content/drive/MyDrive/LSTM_P1/'
    
    # 准备日志相关
    logger = init_logger()
    tr_loss = AverageMeter()
    tr_acc = AverageMeter()
    acc = Accuracy(topK=1)
    # loss_func = torch.nn.BCEWithLogitsLoss()
    loss_func = torch.nn.BCELoss()
    
    # TODO: 实例化数据集，根据自己前面写的参数填写代码，同学可以利用torch.utils.data.random_split()接口，改成交叉验证
    train_dataset = NewsDataset(vocab=vocab, 
                                data_path='/content/drive/MyDrive/LSTM_P1/test_data.csv', 
                                max_len=10, 
                                labels_to_indices=labels_to_indices)
    test_dataset = NewsDataset(vocab=vocab, 
                                data_path='/content/drive/MyDrive/LSTM_P1/test_data.csv', 
                                max_len=10, 
                                labels_to_indices=labels_to_indices)
    
    total_steps = len(train_dataset) // batch_size + (1 if len(train_dataset) % batch_size != 0 else 0)
    
    # 做一下data loader
    train_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)
    test_data_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=True)
    
    # 实例化模型
    model = Classifier_2(config,embedding_matrix)
    
    # device=torch.device("cpu")代表的使用cpu，而device=torch.device("cuda")则代表的使用GPU
    if torch.cuda.is_available():
        device = torch.device("cuda")
    else:
        device = torch.device("cpu")
    model.to(device)
    
    step = 0
    
    # TODO: 定义优化器，有兴趣的同学也可以定义lr_scheduler，可以尝试各种优化器，比如SGD、RMSProp、Adam、AdamW等
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
    scheduler=torch.optim.lr_scheduler.StepLR(optimizer,step_size=2000,gamma=0.9) 
    
    # 开始训练
    print("starting to training ......")
    for epoch in range(1, epochs + 1):
        model.train()
        model.zero_grad()
        for step,batch in enumerate(train_data_loader):
            batch = tuple(t.to(device) for t in batch)
            # TODO: 执行前向过程，计算loss、logits等
            x = batch[0]
            logits,_ = model(x)  
            label_real = batch[1]
            optimizer.zero_grad()
            if step == 0:
              print('logits',logits.shape)
              print('label_real',label_real.shape)
            loss = loss_func(logits, label_real)
            tr_loss.update(loss.item(),n=1)

            label_real_sparse = batch[2]
            acc(logits, label_real_sparse)
            tr_acc.update(acc.value())
            
            # TODO: 反向传播，计算梯度，并更新模型参数，如定义了lr_scheduler更新学习率，之后清空梯度                  
            loss.backward() 
            optimizer.step()
            scheduler.step()
            
            l_r = round(optimizer.param_groups[0]['lr'], 7)
            if (step + 1) % num_eval_steps == 0:
                logger.info(f"[{epoch}/{epochs}] [{step+1}/{total_steps}] loss: {tr_loss.avg:g}, accuracy: {tr_acc.avg:g}, learning_rate: {l_r}")
                
            if (step + 1) % num_save_steps == 0:
                ckpt_dir = f"{output_dir}/checkpoint-{step+1}"
                if os.path.exists(ckpt_dir) is False:
                    os.makedirs(ckpt_dir, exist_ok=True)
                torch.save(model, f"{ckpt_dir}/model.bin")
                acc.reset()
                tr_acc.reset()
                tr_loss.reset()
                
                # 保存标签配置，以免标签变动后重新加载模型出错
                # with open(label_path, "w", encoding="utf-8", errors="ignore") as fp:
                #     for i in range(len(indices_to_labels)):
                #         print(labels_to_indices[i], file=fp)
                
                # # 保存当前模型配置
                # with open(model_save_path, "w") as fp:
                #     json.dump(config, fp)
               
        model.eval()
        tr_acc.reset()
        acc.reset()
        print("starting to testing ......")
        for step, batch in enumerate(test_data_loader):
            batch = tuple(t.to(device) for t in batch)
            # TODO: 执行评估过程，调用前向的时候可以使用with torch.no_grad()修饰，让torch不计算梯度
            x = batch[0]
            label_real = batch[1]
            label_real_sparse = batch[2]
            with torch.no_grad():
                logits,_  = model(x)
      
            acc(logits, label_real_sparse)
            tr_acc.update(acc.value())
            if (step + 1) % num_eval_steps == 0:
                logger.info(f"[{epoch}/{epochs}] accuracy: {tr_acc.avg:g}")
                tr_acc.reset()
        
        ckpt_dir = f"{output_dir}/checkpoint-{step}"
        if os.path.exists(ckpt_dir) is False:
            os.makedirs(ckpt_dir, exist_ok=True)
        torch.save(model, f"{ckpt_dir}/model.bin")
        acc.reset()
        tr_acc.reset()
        tr_loss.reset()

    print('finishing......')

train(config)